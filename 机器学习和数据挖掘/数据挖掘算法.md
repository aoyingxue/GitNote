## 算法：基本方法
### 1. 推断基本规则
#### 1. 1 规则 (1-rule, 1R，OneR规则)
 1. 建立一个只对单一属性进行测试的规则，并将其应用于不同的分支，每个分支对应一个不同的属性值。分支的类就是训练数据在这个分支上出现最多的类。
 2. 很容易计算出规则的误差率，只要计算在训练数据上产生的错误，即统计不属于多数类的实例数量。
 3. 伪代码：
  ```
  对于每一个属性
    对于该属性的每个属性值，按如下方式产生一条规则
        计算每个类别出现的次数
        找出最频繁的类别
        产生一条规则将该类别分配给该属性值
    计算规则的误差率
  选择误差率最小的规则
  ```
[OneR算法简介](https://blog.csdn.net/baidu_25555389/article/details/73379036)

  4. 适用于缺失值和数值属性：把缺失作为另一个属性值。
  5. ==离散化方法==：
     1. 将数值属性转换成名目属性。将训练样本按照数值属性的值进行排序，产生一个类值的序列;离散化通过在这个序列中放置断点来达到分割的目的。
     2. 问题：有可能形成大量的类别范畴。1R算法将自然地倾向于选择能被分裂成很多区间的属性，因为数据集会被分裂成很多部分，所以实例与它们各自所在部分的多数类同属一类的可能性增大。当一个属性存在大量可能值，过度拟合可能会发生。
#### 2. 简单概率模型： ==朴素贝叶斯==
 1. 对一个给定的类使用所有属性，让他们对决策作出同等重要、彼此独立的贡献。
 2. 基于贝叶斯规则并“朴素”地假设 ==（属性）独立== ——只有当事件彼此独立时，概率相乘才是有效的。
 3. 如果某个属性值没有联合每一个类值一起出现在训练集中，那么朴素贝叶斯法将会出错。（概率必须非0，否这0概率超过其他概率掌握了否决权）
 4. 补偿：在分子上加1,分母上加3进行补偿，保证当一个属性值出现0次时，得到一个很小但非0的概率。（拉普拉斯估计器 Laplace estimator：在每个计数结果上加1.一般可以用一个很小的常量μ来代替)
 $$\frac{2+μ/3}{9+μ}$$
 5. 原始的“先验”分布决定了先验信息的重要性，当新的线索来自训练集时，将这些信息考虑在内，他们可以被更新为“后验”分布。如果先验分布具有一种特殊的形式“狄利克雷”，则后验分布具有相同的形式。
 6. 后验分布的均值是由先验分布计算得到的。
 7. 在文档分类方面，朴素贝叶斯的处理速度快而且非常准确。常用==多项式朴素贝叶斯 multinominal Naive Bayes==
 8. 弊端：由于属性之间被认为是完全独立的，所以一些冗余的属性会破坏机器学习过程。比如，如果在天气数据中加入一个新的属性，该属性拥有与属性temperature相同的值，那么temperature影响力会增加，其概率被平方，在最后的决策上具有更大的影响力。如果有十个这样的属性，则最后的决策仅根据属性temperature作出。==属性之间的依赖性==不可避免地会降低朴素贝叶斯识别数据中究竟发生什么的能力。
    对于数值属性，正态分布的假设是朴素贝叶斯的限制，许多属性值并不呈正态分布。如果知道特定属性可能遵循其他分布方式，可以使用那种分布形式的标准估计过程。如果数值分布不是正态分布，又不知道真正的分布，可以使用==核密度估计 kernel density estimation==