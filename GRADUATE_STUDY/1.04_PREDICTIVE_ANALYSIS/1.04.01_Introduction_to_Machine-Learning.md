## I. What is machine learning?

- Relationship between *Y<sub>i</sub> and X<sub>i</sub>*,

![image-20190826111731934](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826111731934.png)

- **Goal** of Machine Learning: Estimate *f*,
- **Estimated** function is $\hat{f}$
- Other Names for Machine Learning: Statistical Learning, Data Mining, etc.
- Hope the random error not too big

- [x] ![image-20190826115709611](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826115709611.png)

## II. Effects of Standard Deviation 

The **irreducible error**: The **variance** of random error 

The difficulty of estimating f will depend on the **standard deviation of the ε’s**.

## III. The Purpose of Machine Learning

- Estimate unknown (True) function *f*.
- Machine-Learning “<u>learns</u>” about *f* from data.
- **Why do we care about estimating** ***f***?
- 2 **reasons** for estimating *f*,
  - **Prediction**: forecast 
  - **Inference**: generalize the system

1. ### Prediction 

   1. **Good estimate for *f***
   2. **Variance of ε is not too large**
   3. Accurate predictions for the response, Y, based on a new value of X

2. ### Inference 

   1. Alternatively, we may also be interested in the type of relationship between Y and the X's.
   2. For example,
      1. **Which particular predictors actually affect the response**? 
      2. Is the relationship **positive or negative**? 
      3. Is the relationship a simple **linear one or is it more complicated** etc.?

### Example: Housing Inference

![image-20190826115527232](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826115527232.png)

- **Accurate in predicting**
- **But not good for inferences** (because of the black box)

- [x] ![image-20190826115730505](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826115730505.png)

- [ ] ![image-20190826115832885](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826115832885.png)

## IV. Learning Methods

#### 1. How Do We Estimate f?

- Training data
  - ![image-20190826120124330](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826120124330.png)
- **Use the training data** and **a learning method** to estimate *f*.
- Learning Methods:
  - **Parametric Methods (pre-determined structure)**
  - **Non-parametric Methods (flexible, algorithm allows any shape)**

#### 2. Parametric methods

- Shape is **predetermined**.

- To Estimate *f*, we estimate a set of parameters.

- They involve a two-step model based approach

  - STEP 1

    Make some **assumption about the functional form** of *f*, i.e. come up with a model. 

    Linear model i.e. (Linear regression)

    ![image-20190826120727932](/Users/aoyingxue/Library/Application Support/typora-user-images/image-20190826120727932.png)

  - STEP 2

    - Use training data to estimate *f* or equivalently the unknown **parameters** such as *β<sub>0</sub>*,*β<sub>1</sub>*,*β<sub>2</sub>*,…,*β*<sub>p</sub>.
    - Estimated Betas are "$\hat{β}_0,\hat{β}_1,\hat{β}_2,…,\hat{β}_p.$"
    - The most common approach for estimating the parameters in a linear model is ordinary least squares (OLS).
    - For the same data, we get the same estimate.

#### 3. Non-parametric methods

- No **assumptions about function** **form **of *f*.
- There are **tuning parameters**.
- <u>Advantages</u>: They **accurately fit** a wider range of possible shapes of f.
  - Avoid the assumption of a particular functional form for f
- <u>Disadvantages</u>: A very **large number of observations** is required to obtain an accurate estimate of f. They can also over fit the data. 
- Non-linear regression methods are **more flexible** and can potentially provide more accurate estimates. But sometimes it will fit for all the data but perform poorly to predict

## V. Prediction Accuracy vs. Model Interpretability

- **Why not just use a more flexible** method?

  - **Linear** **regression** is much **easier to interpret** (the Inference part is better). For example, in a linear model, *β*<sub>j</sub> is the average increase in *Y* for a one unit increase in *X*<sub>j</sub> holding all other variables constant.
  - It is often possible to get more **accurate** predictions with a **simple**, instead of a complicated, model because it **is harder to fit a more flexible model**.

  The flexible model will touch every data point = **overfitting** 

## VI. Supervised vs. Unsupervised Learning

- Two learning problems: Supervised and Unsupervised

#### Supervised Learning

- Both the **predictors, X<sub>i</sub>,** and the **response, Y<sub>i</sub>,** (<u>target variable</u>) are observed. 
- Learning is <u>guided</u> by *Y* variable.
- Example: Linear Regression.
- Primary focus is supervised learning.

#### Unsupervised Learning

- In this situation only the ***X****i*’s are observed. 

- **No response** *Y* variable.

- A common unsupervised learning is **clustering**.

- Example: **Market Segmentation** - divide potential customers into groups.

  ![image-20190828104849179](image-20190828104849179.png)

## VII. Regression vs. Classification

#### Supervised Learning: **Regression and Classification**

#### Regression

- ***Y*** **is continuous/numerical**. *X* can be <u>continuous or categorical</u>

- Example: 

  - Boston Data Set

       *Y*: Median value of owner-occupied homes

       ***X***: per capita crime rate, average number of rooms, pupil-to-teacher ratio, etc.

  - Hitters Data Set (baseball, 1986 and 1987) 

       *Y*: 1987 annual salary on opening day

       ***X***: number of home-runs in 1986, number of hits in 1986, number of runs in 1986, etc.  

#### Classification

- ***Y*** **is categorical**. *X* can be <u>continuous or categorical</u>

- Examples:

  - Orange Juice Data

       *Y*: Whether the customer bought Citrus Hill or Minute Maid (Binary)

       ***X***: store id, price of CH, price of MM, etc.

  - Credit Card Default Data

       *Y*: Whether the customer defaulted on their Debt or not

       ***X***: whether the customer is student, average balance on the credit card, and income.   

#### Algorithms

- Regression and Classification: **Trees, Random Forest, Boosted Trees, Neural Network**, etc.
- Regression: **Linear Regression, Ridge Regression**, least absolute shrinkage and selection operator (**LASSO**), etc.
- Classification: **Logistic Regression**, Linear Discriminant Analysis (**LDA**), Quadratic Discriminant Analysis (**QDA**), etc.

## VIII. Training vs. Test Data

#### Classical Approach

- Use **all** data for **Training**
- Use **Model** fit measures to measure accuracy
- Example: $R^2$

#### Modern Approach

![image-20190828115923524](image-20190828115923524.png)

- Training **part** is used to build model 
- **<u>Validation data</u>** is used to access the model performance
- Example: **MSE** (Training MSE; Test MSE) = mean square error (calculated both the training side and test side of data, whichever model ends up with the **least test error** will be declared as the winner); 

## IX. Different Names for the Same Things

- *Y* variable: **Response, Target, Outcome, and <u>Dependent</u> Variable**. 

- *X* variable: **<u>Independent</u> and Predictor** 

- Machine-Learning: **Statistical Learning** (Statisticians), Data Mining (engineers and analysts), Artificial Intelligence (computer scientists), etc.

