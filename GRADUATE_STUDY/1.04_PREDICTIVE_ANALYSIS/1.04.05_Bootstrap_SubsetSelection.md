Quiz: **Exercise 5.4: Problem 2 (leave out g & h);** **Exercise 6.8: Problem 1.** 

## 5a.1: Purpose of Bootstrap

- Cross Validation: primarily used for estimating test error
- Bootstrap: principally used to estimate **various measures of error** or uncertainty of parameter estimates; for example, **standard error of parameter estimates**, confidence intervals for parameters.
  - One of the most important techniques in all of machine-learning
  - Widely applicable, extremely powerful, computer intensive method.

<img src="1.04.05_Bootstrap.assets/image-20190921200953147.png" alt="image-20190921200953147" style="zoom:50%;" />

[^answer]: it should be true instead of false 

## 5a.2 Classical Approach for Estimating Standard Errors

- **Sample Variance**: $\hat{σ}^2=\frac{1}{n-1}\sum\limits_{i=1}^n(x_i-\bar{x})^2 $

- What is the Standard Error (SE) of $\hatσ^2$?

- **Assume** that $x_1, …, x_n$ are i.i.d.(independent, identical) normally distributed.
- $Var(\hatσ^2 )=  \frac{2σ^4}{n-1}$
- $SE(\hatσ^2 )=  \frac{\sqrt{2}σ^2}{\sqrt{n-1}}$

- Those formulas are good only under the assumed conditions

- The classical approach works for certain statistics under specific modeling assumptions. However, what happens if:
- The modeling assumptions — for example, $x_1, …, x_n$ are i.i.d being normal — break down?
- The estimator does not have a simple form and its sampling distribution cannot be derived analytically?

Bootstrap is always the better solution.

## 5a.3 Bootstrap Process

#### Example: Two Financial Assets

- Two Financial Assets: Yield Returns of *X* and *Y*.
- α in *X* and 1- α in *Y.*
- *Minimize* Risk or Variance of the investment.
- i.e.,*Minimize*
- Solution:<img src="1.04.05_Bootstrap.assets/image-20190921203143458.png" alt="image-20190921203143458" style="zoom:50%;" />

- <img src="1.04.05_Bootstrap.assets/image-20190921203155490.png" alt="image-20190921203155490" style="zoom:50%;" />

#### Standard Error Calculation

Mean of the alpha’s:

$\bar{a}=\frac{1}{B}\sum\limits_{i=1}^B\hatα^{∗i}$

Standard Error of the alpha’s:

$SE(\hatα)=\sqrt{\frac{1}{B-1}\sum\limits_{i=1}^B{(\hatα^{∗I}-\bar{a})^2}}$

## 5b.1 Purpose of Subset Selection

#### Improving on the Least Squares Regression Estimates? 

We want to improve the Linear Regression model, by replacing the least square fitting with some alternative fitting procedure, i.e., the values that minimize the test mean square error (MSE)

There are 2 reasons we might not prefer to just use the ordinary least squares (OLS) estimates

**1. Prediction Accuracy**

**2. Model Interpretability** 

#### 1. Prediction Accuracy

- The **least squares** estimates have relatively **low bias and low variability** especially when the relationship between Y and X is **linear** and the **number** of observations n is way **bigger** than the number of predictors p 
- But, when **n≈p** , then the least squares fit can have **high variance **and may result in over fitting and poor estimates on unseen observations
- And, when n<p , then the variability of the least squares fit increases dramatically, and the **variance **of these estimates is **infinite** 

#### 2. Model Interpretability

- When we have a large number of variables X in the model there will generally be many that have **little or no effect **on Y
- Leaving these variables in the model makes it harder to see the “**big picture**”, i.e., the effect of the “**important variables**”
- The model would be easier to interpret by removing (i.e. setting the **coefficients to zero**) the unimportant variables

#### Solution

- Subset Selection
  - Identify a subset of p predictors and then fit the model using the subset
  - e.g. best subset selection and stepwise selection
- Shrinkage/Regularization
  - Involves shrinking the estimates **coefficients towards zero**
    - Effective in reducing the variance of the model at the cost of increasing the bias
  - This **shrinkage reduces the variance**
  - Some of the coefficients may shrink to exactly zero, and hence shrinkage methods can also perform variable selection
  - e.g. Ridge regression and the Lasso
- Dimension Reduction
  - Involves projecting all p predictors into an **M-dimensional space where M < p**, and then fitting linear regression model
  - e.g. Principle Components Regression  

## 5b.2 Measures for Model Comparison

- Traditionally, $R^2$ is used in Linear Regression
  - $R^2$ Will get bigger when adding more predictors so we need more approaches that are sensitive to the number of predictors to compare models with different numbers of predictors in it
- To **compare models with different number of predictors**, we can use **other approaches**:
  - Adjusted $R^2$
  - AIC (Akaike information criterion)
  - BIC (Bayesian information criterion)
  - $C_p$ 

- These methods add penalty to **RSS** for the number of variables (i.e. complexity) in the model
  - Because when we add more predictors RSS will go down
- None are perfect

#### Adjusted $R^2$ 

- $Adjusted\ R^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$ (Only difference is "d")
- ***d -* number of predictors in the model**
- **Maximizing** Adjusted $R^2$ is equivalent to **minimizing** $RSS⁄(n-d-1)$
- ✭✭**Adding** insignificant predictors will increase d and increase $RSS⁄(n-d-1)$ resulting in a **decrease** of Adjusted $R^2$
- We want a **<u>large</u>** value for an adjusted $R^2$ 

#### $C_p$, AIC, and BIC

-  $C_p=\frac{1}{n}(RSS+2d\hat\sigma^2)$ 
- $AIC=\frac{1}{n\hat\sigma^2}(RSS+2d\hat\sigma^2)$ 
- $BIS=\frac{1}{n}(RSS+log(n)d\hat\sigma^2)$ 

- When adding predictors, RSS will drop. But adding insignificant predictors will reduce RSS only a little bit. The term with d will increase. Overall, we want $C_p$, AIC, and BIC to be **<u>small</u>**, so adding insignificant predictors will be discouraged by using these measures.

#### Credit Data: $C_p,\ BIC\ \&\ Adjusted\ R^2$ 

- A **small** value of $C_p$ and BIC indicates a **low error**, and thus a better model
- A **large** value for the Adjusted $R^2$ indicates a better model
- ![image-20190922135501942](1.04.05_Bootstrap.assets/image-20190922135501942.png)

## 5b.3 Best Subset Selection

- In this approach, we run a linear regression for each possible combination of the *p* predictors 
- Suppose, we have $X_1, X_2, …, X_p$ potential predictors
- First, we find the best **one predictor model** based on **smallest RSS or the largest $R^2$ **
  - There are **p** possible one predictor models (因为有p个predictor，用单个predictor去fit model，所以最后有p种)
- Then, we find the best two predictors model smallest RSS or largest $R^2$
  - There are $(p-1)p/2$ two predictors models 
- Keep doing the same for **all possible number of predictors** in the model until we build the full model. 

-----------

- One simple approach is to take the subset with the smallest RSS or the largest R2
- **Unfortunately, one can show that the model that includes all the predictors will always have the largest $R^2$ (and smallest RSS)** 
  - So we cannot choose the final model using **$R^2$ (and RSS)** 

- To compare models with **different number of predictors**, we should use: (choose the final model)
  - Test MSE
  - Adjusted $R^2$ 
  - AIC (Akaikeinformation criterion)
  - BIC (Bayesian information criterion)
  - $C_p$ 

## 5b.4 Forward Stepwise Selection

#### Why Not Best Subset Selection

- Best Subset Selection is **computationally intensive** especially when we have a large number of predictors (large *p*)
- Other Faster Options:
  - Forward Stepwise Selection
  - Backward Stepwise Selection

#### Forward Stepwise Selection

- Suppose, we have *$X_1$*, *$X_2$*, …, $X_p$ potential predictors. We add one predictor at a time that improves the model the most until no further improvement is possible. **Predictors that are added will stay in the model.**  
- First, we find the best one predictor model based on smallest RSS or largest $R^2$
  - There are **p** possible one predictor models
- Then, we add another predictor to the existing model that most decreases RSS or increases $R^2$ 
  - There are $(p-1)p/2$ two predictors models 
  - <img src="1.04.05_Bootstrap.assets/image-20190922143946450.png" alt="image-20190922143946450" style="zoom:50%;" />
- Keep adding one predictor at a time that most improves the model until we build the full model. 
  - <img src="1.04.05_Bootstrap.assets/image-20190922144053599.png" alt="image-20190922144053599" style="zoom:50%;" />

- To compare models with **different number of predictors**, we should use: (choose the final model)
  - Test MSE
  - Adjusted $R^2$
  - AIC (Akaikeinformation criterion)
  - BIC (Bayesian information criterion)
  - $C_p$ 

## 5b.5 Backward Stepwise Selection 

- Suppose, we have *$X_1$*, *$X_2$*, …, $X_p$ potential predictors. Starting from the full model, we **remove** one predictor at a time that causes the **least deterioration to the model accuracy**.

- First, we start with the full model.
  - <img src="1.04.05_Bootstrap.assets/image-20190922144353557.png" alt="image-20190922144353557" style="zoom:50%;" />

- Then, we remove a predictor from the existing model that causes **the least decline in the accuracy** of the model
  - There are **p** possible one predictor models
- Keep removing one predictor at a time that causes the least deterioration to the model until we have only one predictor in the model. 
  - <img src="1.04.05_Bootstrap.assets/image-20190922144429837.png" alt="image-20190922144429837" style="zoom:50%;" />

- To compare models with **different number of predictors**, we should use: (choose the final model)
  - Test MSE
  - Adjusted $R^2$
  - AIC (Akaikeinformation criterion)
  - BIC (Bayesian information criterion)
  - $C_p$ 

## Lab

#### Bootstrap 

```R
library(boot)
#bootstrap using alpha function
boot(Portfolio,alpha.fn, R=1000)
```

#### Best subset selection

```R
library(leaps)
#best subset selection
regfit.full = regsubsets(Salary~.,Hitters) #number of default predictors is 8

regfit.full = regsubsets(Salary~.,Hitters,nvmax = 19) ##default nvmax is 8
reg.summary = summary(regfit.full)

names(reg.summary)
>>[1] "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj" 
```

rsq=$R^2$ 

rss=Residual sum squared

adjr2=adjusted $R^2$

cp=$C_p$ 

bic=BIC

#### Choose the best number of predictors (e.g. by Bic)

```
plot(reg.summary$bic,xlab="Number of Predictors", ylab = "BIC", type = "l")

which.min(reg.summary$bic)

points(6,reg.summary$bic[6],col="red",cex=2,pch=20)

#show the coefficients
coef(regfit.full,6)
```

```R
plot(regfit.full,scale="r2") #r2越大，参与的参数越多
plot(regfit.full,scale="adjr2") #当没用的参数添加进来之后adjr2会下降
plot(regfit.full,scale="Cp") 
plot(regfit.full,scale="bic")
```

<img src="1.04.05_Bootstrap.assets/image-20190922151948239.png" alt="image-20190922151948239" style="zoom:50%;" />

[^注意]: scale的内容一定要打引号

<img src="1.04.05_Bootstrap.assets/image-20190922152335322.png" alt="image-20190922152335322" style="zoom:50%;" />

<img src="1.04.05_Bootstrap.assets/image-20190922152345863.png" alt="image-20190922152345863" style="zoom:50%;" />

<img src="1.04.05_Bootstrap.assets/image-20190922152357219.png" alt="image-20190922152357219" style="zoom:50%;" />

[^Cp / bic]: 值越小，越top，model越准确

#### Forward and Backward Stepwise Selection

```R
regfit.fwd = regsubsets(Salary~.,data = Hitters, nvmax = 19, method = "forward")
regfit.bwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward") #but summary shows in a reversed selection order

#seven predictor model
coef(regfit.full,7)

coef(regfit.fwd,7)

coef(regfit.bwd,7)
```

We will not get the consistent result by 3 model selection methods.