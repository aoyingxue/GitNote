<img src="1.02.04_Segmentation.assets/image-20190918150154091.png" alt="image-20190918150154091" style="zoom:30%;" />

## An intro to cluster analysis

- Main Cluster analysis principle:
  - maximize the between-cluster variance
  - minimize the within cluster variance

- **Every** person is allocated to **one cluster**

#### Decisions to make

- Which clustering variables to include?
  - Age(narrow spread) and income(0-1000000); age won't matter that much; put the variables on same scale⬇︎
- How to scale the cluster variables?
- Which cluster method? (K-means, hierarchy, hybrid)
- How many clusters? 
- Profiling the clusters

## K-means clustering

- K-means clustering is most used technique

- Iterative approach
  - Allocate each observation to closest cluster center 
  - Find center of new clusters
  - Repeat until cluster division is stable

- The number of clusters is chosen by you

#### Procedures 

- Choose centers of 3 clusters randomly (take 3 seeds)
- Note: bad choice on purpose
- Allocate each point to its nearest center
- Find the centroids of the clusters from iteration 1
- Allocate each point to the centroid that is closest to it
- Find the centroids of the clusters from iteration 2.
- Nothing changed!!
- Voila, you’re done

#### Segmentation = simplification

**Segment locations** = centroids: The market is summarized with three “representative” customers

**Segment size**: We know how many real customers are similar to each representative

<img src="1.02.04_Segmentation.assets/image-20190918151343682.png" alt="image-20190918151343682" style="zoom:50%;" />

#### Technical problems

- K-means segmentation has a **random** seed
  - Output is somewhat random
  - Remedies
    - High number of trials (pick up the one statistically performed best)
    - **“Smart” choice of initial seeds** 
    - Start at Hierarchical Clustering outcome

## Hierarchical Clustering

- Each data point starts as its own cluster
- Combine the **2 closest** points into a cluster (with a **location at the centroid**)
- Continue until the desired number of clusters remain

#### Technical problems

- Stable – **no randomness**
- Works well in general
- However, in real data (vs. artificially divided data in picture), it can give clusters that are not the most compact
  - Advantage of K-means
- Our solution with the software:
  - **Use hierarchical to get starting seeds for K-means** – Run K-means from there
  - Gives us the best of both worlds

