### MapReduce  

- Two key components in MapReduce:
  - A mapper function
  - A reduce function
- We want to run Python on both functions by using the Unix piping concept
  - The main idea is for debugging purposes
  - Stdin and stdout are used for both functions
- Once both functions were tested, all you have to do is to specify the input file and output folder in the HDFS and run it

![image-20191112113853768](1.07.04_Python%20in%20Hadoop.assets/image-20191112113853768.png)

- **Mapper** Steps:
  - Split Lines.
  - Split Words.
  - Count the words.

<img src="1.07.04_Python%20in%20Hadoop.assets/image-20191112113913695.png" alt="image-20191112113913695" style="zoom:70%;" />

#### example

- The **reduce** function in Python (reducer.py):

![image-20191112113944003](1.07.04_Python%20in%20Hadoop.assets/image-20191112113944003.png)

#### Debugging in Unix (not in HDFS yet)  

- Make both mapper.py and reducer.py executable
  - **chmod** **+x mapper.py**
  - **chmod** **+x reducer.py**
- Execute both codes using standard IO
  - –**cat** **text.txt** **| python mapper.py | python reducer.py**

#### Execute MapReduce in HDFS  

- First put the file in HDFS
  - **hdfs dfs -put 1987.csv** 

- Create a bash file called runmr.sh

  ![image-20191112114053367](1.07.04_Python%20in%20Hadoop.assets/image-20191112114053367.png)

- Run this bash file by
  - –**bash runmr.sh**
  - **Make sure to provide correct address to all files and folders.** 

![image-20191112125322264](1.07.04_Python%20in%20Hadoop.assets/image-20191112125322264.png)

